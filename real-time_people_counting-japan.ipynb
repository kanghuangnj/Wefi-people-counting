{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time_People_Counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import glob\n",
    "import spark\n",
    "import pandas as pd\n",
    "import functools\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import scipy as spy\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from tqdm import tqdm\n",
    "\n",
    "pj = os.path.join\n",
    "datapath = '/Users/kanghuang/Documents/work/WeFi/data/japan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clientId', 'campusId', 'buildingId', 'floorId', 'floorNumber', 'apMac',\n",
      "       'apManufacturer', 'deviceMac', 'manufacturerId', 'timeLocal',\n",
      "       'timezone', 'date', 'isApple', 'knownOUI', 'rssi', 'ssid'],\n",
      "      dtype='object')\n",
      "Index(['clientId', 'campusId', 'buildingId', 'floorId', 'floorNumber', 'apMac',\n",
      "       'apManufacturer', 'deviceMac', 'manufacturerId', 'timeLocal',\n",
      "       'timezone', 'date', 'isApple', 'knownOUI', 'rssi', 'ssid'],\n",
      "      dtype='object')\n",
      "Index(['clientId', 'campusId', 'buildingId', 'floorId', 'floorNumber', 'apMac',\n",
      "       'apManufacturer', 'deviceMac', 'manufacturerId', 'timeLocal',\n",
      "       'timezone', 'date', 'isApple', 'knownOUI', 'rssi', 'ssid'],\n",
      "      dtype='object')\n",
      "Index(['clientId', 'campusId', 'buildingId', 'floorId', 'floorNumber', 'apMac',\n",
      "       'apManufacturer', 'deviceMac', 'manufacturerId', 'timeLocal',\n",
      "       'timezone', 'date', 'isApple', 'knownOUI', 'rssi', 'ssid'],\n",
      "      dtype='object')\n",
      "578\n"
     ]
    }
   ],
   "source": [
    "def get_ap_log(bids, days=60):\n",
    "    df_list = []\n",
    "    c = 0\n",
    "    cur_date = pd.to_datetime('now')\n",
    "    start_date = cur_date - timedelta(days=days)\n",
    "    start_date = start_date.replace(minute=0, hour=0, second=0)\n",
    "    end_date = cur_date.replace(minute=0, hour=0, second=0)\n",
    "    for path in glob.glob(pj(datapath, '*/*.parquet')):\n",
    "        ap_log = pd.read_parquet(path)\n",
    "        try:\n",
    "            ap_log['localTimestamp'] = pd.to_datetime(ap_log['localTimestamp'])\n",
    "            ap_log = ap_log[(ap_log['buildingId'].isin(bids)) \n",
    "#                            &(ap_log['localTimestamp'] > start_date) \n",
    "#                            &(ap_log['localTimestamp'] < end_date)\n",
    "                           &(ap_log['ssid'] != \"\")]\n",
    "            if len(ap_log) == 0:\n",
    "                c += 1\n",
    "            df_list.append(ap_log)\n",
    "        except:\n",
    "            print (ap_log.columns)\n",
    "    df = pd.concat(df_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "bids = [\n",
    "    #27, #'80654332-e7df-11e8-b223-0626947414d8'\n",
    "    22, # '4551855b-7eee-4fb4-8f43-5756c8dde881' \n",
    "]\n",
    "\n",
    "all_ap_df = get_ap_log(set(bids))\n",
    "\n",
    "# 'abnormal'\n",
    "# ae301ff2d6f111ea8bde16245ec60c24, 5ad0f61cd55f11ea8bde16245ec60c24, acf38bb0d6f111ea95420242ac130004,\n",
    "# 83118d7ad62811eaaaea0242ac130004, 2ef054dad49611ea8bde16245ec60c24, 584d7852d55f11ea8bde16245ec60c24,\n",
    "# 2f10947ad49611ea87f90242ac130004, af5b284ad6f111ea835f0242ac130004, 83ca466cd62811ea8bde16245ec60c24,\n",
    "# 81ed97d6d62811ea87ca0242ac130004, f716d86ed69711ea8bde16245ec60c24, 83cc7572d62811ea8bde16245ec60c24,\n",
    "# abce3212d6f111ea8bde16245ec60c24, 02acdcb4d3cd11ea9d7f0242ac130004, 2fe33f38d49611ea8bde16245ec60c24,\n",
    "# 2ca1379ed49611eab69c0242ac130004, fcd7b26ed69c11ea8bde16245ec60c24, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bid = 22\n",
    "valid_ap_df = all_ap_df[(all_ap_df['buildingId'] == bid) & (all_ap_df['floorNumber'] != 999)]\n",
    "valid_ap_df = valid_ap_df.rename(columns={'deviceMac': 'uuid', 'floorNumber': 'floor_number'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ap_df['date'] = pd.to_datetime(valid_ap_df['date'])\n",
    "valid_ap_df['type'] = 'device'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outlier(row):\n",
    "    # overnight\n",
    "    timestamp = pd.to_datetime(row.name)\n",
    "    earlyTimestamp = timestamp.replace(minute=0, hour=6, second=0)\n",
    "    lateTimestamp = timestamp.replace(minute=0, hour=23, second=0)\n",
    "    overnight = (row['localTimestamp'] < earlyTimestamp) | (row['localTimestamp'] > lateTimestamp)\n",
    "    row =  row[~overnight]\n",
    "#     row['date'] = pd.to_datetime(row['date'])\n",
    "#     row['dow'] = row.date.apply(lambda x: x.weekday())\n",
    "#     row = row[row.dow >= 5]\n",
    "    return row\n",
    "\n",
    "keycard_swipes_df = pd.read_csv('keycard_swipes_japan.csv')\n",
    "keycard_swipes_df['date'] = keycard_swipes_df.apply(lambda row: row['KEYCARD_SWIPE_LOCAL'].split()[0], axis=1)\n",
    "keycard_swipes_df = keycard_swipes_df.rename(columns={'KEYCARD_SWIPE_LOCAL': 'localTimestamp', 'CUSTOMER_UUID': 'uuid'})\n",
    "keycard_swipes_df['localTimestamp'] = pd.to_datetime(keycard_swipes_df['localTimestamp'])\n",
    "keycard_swipes_df['date'] = pd.to_datetime(keycard_swipes_df['date'])\n",
    "keycard_swipes_df = keycard_swipes_df[keycard_swipes_df['date'].isin(valid_ap_df.date.unique())]\n",
    "valid_keycard_df = deepcopy(keycard_swipes_df)\n",
    "valid_keycard_df = valid_keycard_df[(valid_keycard_df.READER_NAME != 'Proxy Reader') & (valid_keycard_df.ACTIVITY == 'Access granted')]\n",
    "valid_keycard_df = valid_keycard_df.groupby('date').apply(filter_outlier).reset_index(drop=True)\n",
    "valid_keycard_df['floor_number'] = valid_keycard_df.READER_NAME.str.extract(r'(\\d{2})\\.\\d+').fillna('') \n",
    "valid_keycard_df = valid_keycard_df[valid_keycard_df.floor_number != '']\n",
    "valid_keycard_df['type'] = 'keycard'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stationary(row, keycard_flag):\n",
    "    # overnight\n",
    "    timestamp = pd.to_datetime(row.name)\n",
    "    earlyTimestamp = timestamp.replace(minute=0, hour=6, second=0)\n",
    "    lateTimestamp = timestamp.replace(minute=0, hour=23, second=0)\n",
    "    if keycard_flag:\n",
    "        overnight = (row['localTimestamp'] < row['min']) \n",
    "    else:\n",
    "        overnight = row['localTimestamp'] < earlyTimestamp\n",
    "    row =  row[overnight]\n",
    "#     row['date'] = pd.to_datetime(row['date'])\n",
    "#     row['dow'] = row.date.apply(lambda x: x.weekday())\n",
    "#     row = row[row.dow >= 5]\n",
    "    return row\n",
    "\n",
    "def smallest_stationary_group(ap_log):\n",
    "    ap_log = ap_log.groupby('uuid').apply(lambda row: row['date'].nunique())\n",
    "    day_n = ap_log.max()\n",
    "    ap_log = ap_log[ap_log == day_n]\n",
    "    return ap_log.index.tolist()\n",
    "\n",
    "def largest_stationary_group(ap_log):\n",
    "#     day_n = len(ap_log.date.unique())\n",
    "#     ap_log = ap_log.groupby('uuid').apply(lambda row: row['date'].nunique())\n",
    "#     ap_log = ap_log[ap_log > 0]\n",
    "    return ap_log.index.tolist()\n",
    "\n",
    "def gen_stationary_group(valid_ap_df, group_func, keycard_swipes_df=None):\n",
    "    #overlap, vocab = cooccurrence_model\n",
    "    keycard_flag=False\n",
    "    if keycard_swipes_df is not None:\n",
    "        boundary_df = keycard_swipes_df.groupby('date')['localTimestamp'].agg(['min'])\n",
    "        valid_ap_df = pd.merge(valid_ap_df, boundary_df, on='date')\n",
    "        keycard_flag = True\n",
    "        \n",
    "    overnight_ap_log = valid_ap_df.groupby('date').apply(lambda df: filter_stationary(df, keycard_flag)).reset_index(drop=True)\n",
    "    stationary_group = group_func(overnight_ap_log)\n",
    "    return set(stationary_group)\n",
    "\n",
    "def filter_low_rssi(valid_ap_df):\n",
    "    rssi = -80\n",
    "    rssi_cond = (valid_ap_df['rssi'] < 0) & (valid_ap_df['rssi'] > rssi)\n",
    "    return valid_ap_df[rssi_cond]\n",
    "\n",
    "def filter_weekend(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['dow'] = df.date.apply(lambda x: x.weekday())\n",
    "    return df[df.dow < 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device to device graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function calculate the cooccurrence count within a single timespan.\n",
    "There are two groups to maintain the state, left group records device ids from previous time window,\n",
    "right group records new incoming device ids.\n",
    "There are three conditions to update the cooccurrence pairs:\n",
    "1. left device is from left group, right device is from and only exists in right group without occur in left device's timespan \n",
    "2. left device is from right group, right device is from and only exists in left group.\n",
    "3. both devices are from right group\n",
    "'''\n",
    "def count_cooccurrence(l, r, rr, cooccurrence, obsolete_index, devices):\n",
    "    \"\"\"\n",
    "    l: left index of left group\n",
    "    \"\"\"\n",
    "    left_freq = defaultdict(int)\n",
    "    right_freq = defaultdict(int)\n",
    "    for i in devices[l: r]:\n",
    "        left_freq[i] += 1\n",
    "    for i in devices[r: rr]:\n",
    "        right_freq[i] += 1\n",
    "        \n",
    "    rd = right_freq.keys()\n",
    "    ld = left_freq.keys()\n",
    "    prev = -1\n",
    "    obsolete_devices = set()\n",
    "    for i in rd:\n",
    "        if not i in cooccurrence:\n",
    "            cooccurrence[i] = defaultdict(int)\n",
    "    # condition 1\n",
    "    for i in rd: \n",
    "        if (not i in ld):\n",
    "            for k in range(l, r):\n",
    "                j = devices[k]\n",
    "                st = obsolete_index[k]\n",
    "                if prev != st:\n",
    "                    obsolete_devices = set(devices[st: k]) \n",
    "                    prev = st\n",
    "                if not i in obsolete_devices: \n",
    "                    cooccurrence[j][i] += 1\n",
    "    # condition 2               \n",
    "    for i in rd: \n",
    "        for j in ld:\n",
    "            if not j in rd: \n",
    "                cooccurrence[i][j] += right_freq[i]\n",
    "    \n",
    "    # condition 3\n",
    "    for i in rd:\n",
    "        for j in rd: \n",
    "            if i != j:\n",
    "                cooccurrence[i][j] += right_freq[i]    \n",
    "                \n",
    "\"\"\"\n",
    "This function slides the time window from left to right in steps of timestamps,\n",
    "every time left index will try to shift one timestamp to right direction, and right index will find the \n",
    "right most index within the time window starts from left index.\n",
    "It will also maintain every device left most index within time window\n",
    "\"\"\"                \n",
    "def create_context(timespan, df, cooccurrence):\n",
    "    df = df.sort_values('localTimestamp')\n",
    "    timestamps = df['localTimestamp'].tolist()\n",
    "    devices = df['uuid'].tolist()\n",
    "    l, r = 0, 0 \n",
    "    obsolete_index = []\n",
    "    while True:\n",
    "        ll = l\n",
    "        while ll < r:\n",
    "            assert timestamps[ll] >= lt\n",
    "            if timestamps[ll] != lt:\n",
    "                break\n",
    "            ll += 1\n",
    "        if ll == len(timestamps):\n",
    "            break\n",
    "        l = ll\n",
    "        lt = timestamps[l]\n",
    "        rr = r\n",
    "        while (rr < len(timestamps)) and (timestamps[rr] < lt + timedelta(minutes=timespan)):\n",
    "            rr += 1\n",
    "        if r == rr: continue\n",
    "        obsolete_index.extend([l]*(rr-r))\n",
    "        count_cooccurrence(l, r, rr, cooccurrence, obsolete_index, devices)\n",
    "        r = rr\n",
    "    assert len(obsolete_index) == len(devices)\n",
    "        \n",
    "def cooccurrence_update(metadata, ap_df, timespan, group_columns=['date', 'apMac', 'floor_number']):\n",
    "    cooccurrence, freq = metadata\n",
    "    for mac in ap_df['uuid'].tolist():\n",
    "        freq[mac] += 1\n",
    "    for _, df in ap_df.groupby(group_columns):\n",
    "        create_context(timespan, df, cooccurrence)\n",
    "    return cooccurrence, freq\n",
    "\n",
    "        \n",
    "def cooccurrence_transform(metadata, thresh=0.5, binary=True):\n",
    "    cocur, freq = metadata\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocab = {}\n",
    "    for i in cocur:\n",
    "        vocab.setdefault(i, len(vocab))\n",
    "        for j in cocur[i]:\n",
    "            index = vocab.setdefault(j, len(vocab))\n",
    "            connectivity = cocur[i][j]*1.0 / freq[i]\n",
    "            if binary:\n",
    "                if connectivity > thresh:\n",
    "                    connectivity = 1\n",
    "                else:\n",
    "                    connectivity = 0\n",
    "            if connectivity > 0:\n",
    "                data.append(connectivity)\n",
    "                indices.append(index)\n",
    "        indptr.append(len(indices))\n",
    "    n = len(cocur)\n",
    "    graph = csr_matrix((data, indices, indptr), shape=(n, n), dtype=float)\n",
    "    return graph, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index():\n",
    "    date_start = datetime.datetime(2020, 1, 6)\n",
    "    date_end = datetime.datetime(2020, 3, 24)\n",
    "    date_groups = []\n",
    "    cur_date = date_start\n",
    "    while cur_date <= date_end:\n",
    "        date_groups.append((cur_date, min(date_end, cur_date+timedelta(days=6))))\n",
    "        cur_date += timedelta(days=7)\n",
    "    return date_groups\n",
    "\n",
    "def prepare_data(k, date_groups):\n",
    "    train_ap_df = valid_ap_df[(valid_ap_df.date >= date_groups[k][0]) & (valid_ap_df.date <= date_groups[k][1])]\n",
    "    test_ap_df = valid_ap_df[(valid_ap_df.date >= date_groups[k+1][0]) & (valid_ap_df.date <= date_groups[k+1][1])]\n",
    "    train_keycard_df = valid_keycard_df[(valid_keycard_df.date >= date_groups[k][0]) & (valid_keycard_df.date <= date_groups[k][1])]\n",
    "    test_keycard_df = valid_keycard_df[(valid_keycard_df.date >= date_groups[k+1][0]) & (valid_keycard_df.date <= date_groups[k+1][1])]\n",
    "    return train_ap_df, test_ap_df, train_keycard_df, test_keycard_df\n",
    "\n",
    "def preprocess(ap_df, group_func, keycard_df=None):\n",
    "    key_columns = ['localTimestamp', 'floor_number', 'uuid', 'apMac']\n",
    "    print ('before preprocess %d' % len(ap_df))\n",
    "    ap_df = filter_low_rssi(ap_df)\n",
    "    ap_df = ap_df[~ap_df.duplicated(key_columns)]\n",
    "    stationary_group = gen_stationary_group(ap_df, group_func, keycard_df)\n",
    "    print ('after preprocess %d' % len(ap_df))\n",
    "    return ap_df, stationary_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ssg(df, vocab, labels, static_group):\n",
    "    candidate = defaultdict(int)\n",
    "    uuids = [vocab[i] for i in df.uuid.unique() if i in vocab]\n",
    "    new_device_n = (df.uuid.nunique() - len(uuids)) // 2\n",
    "    for i, label in enumerate(labels):\n",
    "        if i in uuids:\n",
    "            candidate[label] += 1\n",
    "    #print (len(uuids), len(labels))\n",
    "    if len(candidate) == 0:\n",
    "        return new_device_n\n",
    "    c = 0\n",
    "    for label in candidate:\n",
    "        if (not label in static_group) and (candidate[label] <= 3):\n",
    "            c += 1\n",
    "    return c + new_device_n\n",
    "\n",
    "def ssg_predict(train_keycard_df, test_keycard_df, test_ap_df, model, stationary_group): \n",
    "    test_df = filter_low_rssi(test_ap_df)\n",
    "    #test_df = test_df[~test_df.isin(stationary_group)]\n",
    "    estimate = test_df.groupby('date').apply(lambda df: ssg(df, model, stationary_group))\n",
    "    keycard = test_keycard_df.groupby('date').apply(lambda rows: rows.uuid.nunique())\n",
    "    delta = test_keycard_df.groupby('date').apply(lambda rows: rows[~rows.uuid.isin(train_keycard_df.uuid.unique())].uuid.nunique())\n",
    "    device = test_df.groupby('date').apply(lambda rows: rows.uuid.nunique())\n",
    "  \n",
    "    dashboard = pd.concat([estimate, keycard, device, delta], axis=1).reset_index(drop=False)\n",
    "    dashboard = dashboard[~dashboard[0].isna()]\n",
    "    \n",
    "    #truth = pd.read_csv('japan/japan_manual_counts.csv')\n",
    "    return dashboard\n",
    "\n",
    "def get_correlation(df):\n",
    "    pearson1 = df.manual_count.corr(df.estimate)\n",
    "    pearson2 = df.manual_count.corr(df.baseline)\n",
    "    return pearson1, pearson2\n",
    " \n",
    "def get_error_rate(df):\n",
    "    err1 = np.sum((np.abs(df.estimate-df.manual_count)) / (df.manual_count+1)) / len(df)\n",
    "    err2 = np.sum((np.abs(df.baseline-df.manual_count)) / (df.manual_count+1)) / len(df)\n",
    "    return err1, err2\n",
    "\n",
    "def get_mae(df):\n",
    "    mae1 = np.sum(np.abs(df.estimate-df.manual_count)) / len(df)\n",
    "    mae2 = np.sum(np.abs(df.baseline-df.manual_count)) / len(df)\n",
    "    return mae1, mae2\n",
    "\n",
    "def ssg_evaluate(ratio, truth, test_ap_df, model, stationary_ssg, stationary_ratio, common_area_aps=None):  \n",
    "    graph, vocab = model\n",
    "    test_df = filter_low_rssi(test_ap_df)\n",
    "    #test_df = test_df[~test_df.isin(stationary_group)]\n",
    "    truth['time_local'] = pd.to_datetime(truth['time_local'])\n",
    "    truth = truth[(truth['time_local'] >= test_df.date.min()) & (truth['time_local'] <= test_df.date.max())].reset_index(drop=True)\n",
    "    n_components, labels = connected_components(csgraph=graph, connection='strong', return_labels=True)\n",
    "    \n",
    "    static_group = set()\n",
    "    stationary_ssg_ids = set([vocab[mac] for mac in stationary_ssg])\n",
    "    for i, label in enumerate(labels):\n",
    "        if i in stationary_ssg_ids:\n",
    "            static_group.add(label)\n",
    "    if not common_area_aps is None:\n",
    "        truth = truth[truth['floor'].isin(common_area_aps)]\n",
    "        \n",
    "    for i, row in truth.iterrows():\n",
    "        timestamp = row['time_local']\n",
    "        cond = (test_df['localTimestamp'] > (timestamp - timedelta(minutes=5))) & (test_df['localTimestamp'] < (timestamp + timedelta(minutes=5)))\n",
    "        cond &= (test_df['floor_number'] == row['floor'])\n",
    "        if not common_area_aps is None:\n",
    "            cond &= (test_df['apMac'].isin(common_area_aps[row['floor']]))\n",
    "        ap_df = test_df[cond] \n",
    "        truth.loc[i, 'baseline'] = round(ap_df[~ap_df.uuid.isin(stationary_ratio)].uuid.nunique() * ratio)\n",
    "        truth.loc[i, 'estimate'] = ssg(ap_df, vocab, labels, static_group)\n",
    "        truth.loc[i, 'device'] =  ap_df.uuid.nunique() \n",
    "        \n",
    "        #print (ap_df.uuid.nunique(), row['manual_count'], count, timestamp, row['floor'])\n",
    "        \n",
    "    truth = truth.sort_values(['floor', 'time_local']).reset_index(drop=True)\n",
    "    cond = (truth['device'] == 0) & (truth['manual_count'] != 0)\n",
    "    cond |= ((truth['device'] != 0) & (truth['manual_count'] == 0))\n",
    "    cond |= (truth['device'] < truth['manual_count'])\n",
    "    res = truth[~cond]\n",
    "    err1, err2 = get_error_rate(res)\n",
    "    mae1, mae2 = get_mae(res)\n",
    "    pear1, pear2 = get_correlation(res)\n",
    "    return res, err1, err2, mae1, mae2, pear1, pear2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fit(train_ap_df, train_keycard_df):\n",
    "    train_keycard_df = filter_weekend(train_keycard_df)\n",
    "    train_ap_df = filter_weekend(train_ap_df)\n",
    "    train_df, stationary_group = preprocess(train_ap_df, largest_stationary_group)\n",
    "    train_df = train_df[~train_df.uuid.isin(stationary_group)]\n",
    "    pseudo_gold = train_keycard_df.groupby('date').apply(lambda row: row.uuid.nunique())\n",
    "    train = train_df.groupby('date').apply(lambda row: row.uuid.nunique())\n",
    "    y = pseudo_gold.to_numpy().astype('float64') \n",
    "    x = train.to_numpy().astype('float64')\n",
    "    X = np.expand_dims(x, -1)\n",
    "    a, _, _, _ = np.linalg.lstsq(X, y)\n",
    "    return a[0], stationary_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========iteration 0=========\n",
      "before preprocess 4889788\n",
      "after preprocess 4780496\n",
      "stationary group size: 92\n",
      "graph running time: 182s\n",
      "========iteration 1=========\n",
      "before preprocess 6517265\n",
      "after preprocess 5082832\n",
      "stationary group size: 128\n",
      "graph running time: 100s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanghuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/kanghuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before preprocess 3751720\n",
      "after preprocess 3751720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanghuang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation time: 69s\n",
      "metric:  0.2884447586662175 0.2521821271071432 28.361702127659573 24.634042553191488 0.8903468860427809 0.8872975321495068\n",
      "================================\n",
      "========iteration 2=========\n",
      "before preprocess 19662538\n",
      "after preprocess 6169012\n",
      "stationary group size: 142\n",
      "graph running time: 208s\n",
      "before preprocess 5629023\n",
      "after preprocess 5629023\n",
      "evaluation time: 68s\n",
      "metric:  0.2684042320028151 0.22063755158038692 25.94333333333333 19.656666666666666 0.929633752011041 0.9363085836592946\n",
      "================================\n",
      "========iteration 3=========\n",
      "before preprocess 9971054\n",
      "after preprocess 4332157\n",
      "stationary group size: 153\n",
      "graph running time: 162s\n",
      "before preprocess 3864493\n",
      "after preprocess 3864493\n",
      "evaluation time: 34s\n",
      "metric:  0.25038922383608986 0.2361977656001056 28.271966527196653 27.209205020920503 0.9155679287639436 0.9235128683860651\n",
      "================================\n",
      "========iteration 4=========\n",
      "before preprocess 3347030\n",
      "after preprocess 3278246\n",
      "stationary group size: 159\n",
      "graph running time: 113s\n",
      "before preprocess 3057728\n",
      "after preprocess 3057728\n",
      "evaluation time: 44s\n",
      "metric:  0.34900701336364354 0.27585697146701665 38.73714285714286 32.05714285714286 0.8921927643864139 0.9035900514233619\n",
      "================================\n",
      "========iteration 5=========\n",
      "before preprocess 14939162\n",
      "after preprocess 10842088\n",
      "stationary group size: 164\n",
      "graph running time: 347s\n",
      "before preprocess 9465666\n",
      "after preprocess 9465666\n",
      "evaluation time: 94s\n",
      "metric:  0.3477420604004818 0.1871957720080779 38.71666666666667 19.022222222222222 0.9288757259826637 0.9423591628742032\n",
      "================================\n",
      "========iteration 6=========\n",
      "before preprocess 21388412\n",
      "after preprocess 15242913\n",
      "stationary group size: 184\n",
      "graph running time: 653s\n",
      "before preprocess 13719986\n",
      "after preprocess 13719986\n",
      "evaluation time: 133s\n",
      "metric:  0.2479919618558943 0.1888203131906579 25.11731843575419 19.078212290502794 0.8708318630966231 0.9540508156200859\n",
      "================================\n",
      "========iteration 7=========\n",
      "before preprocess 26727207\n",
      "after preprocess 17867749\n",
      "stationary group size: 360\n",
      "graph running time: 771s\n",
      "before preprocess 16414971\n",
      "after preprocess 16414971\n",
      "evaluation time: 141s\n",
      "metric:  0.6493573862043092 0.1833278252153027 72.25 19.489583333333332 0.7574578759254943 0.8960397311750082\n",
      "================================\n",
      "========iteration 8=========\n",
      "before preprocess 22576776\n",
      "after preprocess 11132527\n",
      "stationary group size: 369\n",
      "graph running time: 561s\n",
      "before preprocess 9663586\n",
      "after preprocess 9663586\n",
      "evaluation time: 112s\n",
      "metric:  0.7746853754998484 0.2664782814350525 84.66839378238342 28.207253886010363 0.7983769340649337 0.9175174795897243\n",
      "================================\n",
      "========iteration 9=========\n",
      "before preprocess 28366256\n",
      "after preprocess 19550905\n",
      "stationary group size: 387\n",
      "graph running time: 906s\n",
      "before preprocess 18255247\n",
      "after preprocess 18255247\n",
      "evaluation time: 152s\n",
      "metric:  0.5119675271925836 0.140305290473179 56.80503144654088 14.40251572327044 0.9284731171311046 0.9523187598809412\n",
      "================================\n",
      "========iteration 10=========\n",
      "before preprocess 15396569\n",
      "after preprocess 10295339\n",
      "stationary group size: 401\n",
      "graph running time: 580s\n",
      "before preprocess 8938749\n",
      "after preprocess 8938749\n",
      "evaluation time: 76s\n",
      "metric:  0.779755213264267 0.2755289020366717 80.31666666666666 22.7 0.8941199250082013 0.9371869465195435\n",
      "================================\n",
      "total running time: 4678s\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "truth = pd.read_csv('japan/japan_manual_counts.csv')\n",
    "truth = truth[truth['building_id'] == 22]\n",
    "date_groups = create_index()\n",
    "metadata = {}, defaultdict(int)\n",
    "stationary_global = set()\n",
    "start = time.time()\n",
    "metadata_versions = []\n",
    "test_data = []\n",
    "stationary_ssg_groups = []\n",
    "stationary_ratio_groups = []\n",
    "for k in range(len(date_groups)-1):\n",
    "    print ('========iteration %d=========' % k)\n",
    "    train_ap_df, test_ap_df, train_keycard_df, test_keycard_df = prepare_data(k, date_groups)\n",
    "    train_ap_df, stationary_group = preprocess(train_ap_df, smallest_stationary_group, train_keycard_df)\n",
    "    stationary_global = stationary_global.union(stationary_group)\n",
    "    print ('stationary group size: %d' % len(stationary_global))\n",
    "    metadata = cooccurrence_update(metadata, train_ap_df, 5)\n",
    "    model = cooccurrence_transform(metadata)\n",
    "    print ('graph running time: %ds' % (time.time()-st))\n",
    "    #st = time.time()\n",
    "    #ssg_predict(train_keycard_df, test_keycard_df, test_ap_df, model, stationary_group)\n",
    "    #daily_results.append(res)\n",
    "    #print ('prediction time: %ds' % (time.time()-st))\n",
    "    st = time.time()\n",
    "    if k == 0:\n",
    "        continue\n",
    "    ratio, stationary_ratio_group = lr_fit(train_ap_df, train_keycard_df)\n",
    "    res, err1, err2, mae1, mae2, pear1, pear2 = ssg_evaluate(ratio, truth, test_ap_df, model, stationary_global, stationary_ratio_group) \n",
    "    print ('evaluation time: %ds' % (time.time()-st))\n",
    "    print ('metric: ',  err1, err2, mae1, mae2, pear1, pear2)\n",
    "    print ('================================')\n",
    "    \n",
    "    metadata_versions.append((deepcopy(metadata[0]), deepcopy(metadata[1]), ratio))\n",
    "    test_data.append(test_ap_df)\n",
    "    stationary_ssg_groups.append(deepcopy(stationary_global))\n",
    "    stationary_ratio_groups.append(deepcopy(stationary_ratio_group))\n",
    "print ('total running time: %ds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_area_truth = pd.read_csv('japan/japan_common_area_manual_counts.csv')\n",
    "common_area_ap = {37: ['54:ec:2f:29:e1:e0', '54:ec:2f:2a:0d:10', '54:ec:2f:69:cf:ec', '54:ec:2f:69:f3:bc', '54:ec:2f:6a:09:ac', '54:ec:2f:29:af:b0', '54:ec:2f:2a:0c:40'],\n",
    "                 38: ['54:ec:2f:29:cf:90', '54:ec:2f:29:d6:c0', '54:ec:2f:29:f5:b0', '54:ec:2f:2a:0c:00', '54:ec:2f:29:cf:c0', '54:ec:2f:29:f5:e0', '54:ec:2f:29:f4:20'],\n",
    "                 39: ['54:ec:2f:29:f3:00', '54:ec:2f:29:f4:b0', '54:ec:2f:29:f6:80', '54:ec:2f:29:f5:ad', '54:ec:2f:29:fc:10', '54:ec:2f:2a:01:40'],\n",
    "                 #'40': ['1c:3a:60:4c:5e:80', '1c:3a:60:4c:01:00', '1c:3a:60:4c:01:00', '54:ec:2f:69:e2:60'],\n",
    "                 }\n",
    "for floor in common_area_ap:\n",
    "    uppercase = []\n",
    "    for mac in common_area_ap[floor]:\n",
    "        uppercase.append(mac.upper())\n",
    "    common_area_ap[floor] = uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(k, thresh, common_area_ap=None):\n",
    "    metadata = metadata_versions[k]\n",
    "    test_ap_df = test_data[k]\n",
    "    stationary_ssg = stationary_ssg_groups[k]\n",
    "    stationary_ratio = stationary_ratio_groups[k]\n",
    "    model = cooccurrence_transform(metadata[:2], thresh)\n",
    "    if not common_area_ap is None:\n",
    "        truth_ = truth\n",
    "    else:\n",
    "        truth_ = common_area_truth\n",
    "    return ssg_evaluate(metadata[2], truth_, test_ap_df, model, stationary_ssg, stationary_ratio, common_area_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rounds:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:53<26:36, 53.22s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [01:39<24:43, 51.15s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [02:25<23:10, 49.68s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [03:11<21:53, 48.63s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [03:58<20:46, 47.94s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [04:44<19:46, 47.46s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [05:30<18:50, 47.12s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [06:17<17:57, 46.84s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [07:03<17:06, 46.65s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [07:49<16:18, 46.58s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [08:36<15:29, 46.50s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [09:22<14:42, 46.45s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [10:08<13:55, 46.43s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [10:54<13:07, 46.34s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [11:41<12:21, 46.32s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [12:27<11:35, 46.34s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [13:13<10:48, 46.32s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [14:00<10:01, 46.30s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [14:46<09:16, 46.34s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [15:33<08:30, 46.42s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [16:19<07:45, 46.52s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [17:06<06:59, 46.63s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [17:53<06:13, 46.69s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [18:39<05:26, 46.59s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [19:26<04:39, 46.55s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [20:13<03:52, 46.56s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [20:59<03:06, 46.55s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [21:45<02:19, 46.43s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [22:32<01:32, 46.44s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [23:18<00:46, 46.43s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [24:04<00:00, 46.61s/it]\u001b[A\n",
      "rounds:  10%|█         | 1/10 [24:04<3:36:43, 1444.83s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:46<23:25, 46.86s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [01:32<22:25, 46.41s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [02:17<21:27, 45.97s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [03:01<20:29, 45.52s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [03:46<19:37, 45.29s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [04:31<18:48, 45.12s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [05:15<17:54, 44.75s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [05:59<17:05, 44.60s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [06:43<16:20, 44.58s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [07:28<15:36, 44.60s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [08:11<14:45, 44.26s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [08:56<14:02, 44.36s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [09:40<13:19, 44.40s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [10:24<12:31, 44.21s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [11:08<11:47, 44.20s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [11:52<11:01, 44.11s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [12:37<10:18, 44.21s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [13:21<09:36, 44.32s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [14:06<08:54, 44.55s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [14:51<08:11, 44.66s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [15:36<07:27, 44.70s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [16:20<06:40, 44.50s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [17:04<05:54, 44.37s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [17:48<05:09, 44.23s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [18:33<04:25, 44.30s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [19:17<03:41, 44.26s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [20:01<02:57, 44.37s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [20:46<02:13, 44.37s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [21:30<01:28, 44.28s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [22:14<00:44, 44.17s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [22:57<00:00, 44.44s/it]\u001b[A\n",
      "rounds:  20%|██        | 2/10 [47:02<3:09:57, 1424.71s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:17<08:32, 17.09s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [00:33<08:05, 16.75s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [00:48<07:41, 16.47s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [01:04<07:16, 16.16s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [01:19<06:54, 15.96s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [01:35<06:37, 15.88s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [01:51<06:18, 15.77s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [02:06<06:01, 15.72s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [02:22<05:46, 15.73s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [02:38<05:29, 15.71s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [02:53<05:12, 15.64s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [03:09<04:57, 15.64s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [03:24<04:42, 15.69s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [03:40<04:26, 15.68s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [03:56<04:10, 15.64s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [04:11<03:54, 15.63s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [04:27<03:38, 15.64s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [04:43<03:23, 15.63s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [04:58<03:07, 15.63s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [05:14<02:52, 15.67s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [05:30<02:36, 15.69s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [05:45<02:21, 15.68s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [06:01<02:05, 15.67s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [06:16<01:49, 15.62s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [06:32<01:33, 15.66s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [06:48<01:18, 15.62s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [07:03<01:02, 15.63s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [07:19<00:46, 15.60s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [07:35<00:31, 15.60s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [07:50<00:15, 15.60s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [08:06<00:00, 15.69s/it]\u001b[A\n",
      "rounds:  30%|███       | 3/10 [55:09<2:13:22, 1143.23s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:35<17:41, 35.39s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [01:05<16:17, 33.72s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [01:35<15:12, 32.58s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [02:05<14:22, 31.95s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [02:35<13:36, 31.39s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [03:06<12:58, 31.12s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [03:36<12:19, 30.81s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [04:06<11:44, 30.65s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [04:36<11:09, 30.45s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [05:06<10:37, 30.35s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [05:36<10:06, 30.30s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [06:07<09:34, 30.26s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [06:37<09:06, 30.36s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [07:07<08:36, 30.36s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [07:38<08:05, 30.34s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [08:08<07:34, 30.32s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [08:38<07:04, 30.30s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [09:08<06:32, 30.22s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [09:38<06:02, 30.20s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [10:09<05:32, 30.23s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [10:39<05:03, 30.30s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [11:09<04:32, 30.23s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [11:40<04:01, 30.24s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [12:10<03:31, 30.17s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [12:40<03:01, 30.23s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [13:10<02:31, 30.27s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [13:42<02:02, 30.63s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [14:14<01:32, 30.98s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [14:45<01:02, 31.22s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [15:16<00:31, 31.13s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [15:46<00:00, 30.54s/it]\u001b[A\n",
      "rounds:  40%|████      | 4/10 [1:10:55<1:48:25, 1084.32s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:53<26:35, 53.19s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [01:39<24:40, 51.06s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [02:24<23:01, 49.34s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [03:09<21:39, 48.14s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [03:55<20:30, 47.32s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [04:40<19:28, 46.75s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thresh:  23%|██▎       | 7/31 [05:26<18:33, 46.39s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [06:12<17:42, 46.22s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [06:57<16:51, 45.98s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [07:43<16:03, 45.90s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [08:29<15:17, 45.86s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [09:14<14:27, 45.66s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [09:59<13:41, 45.65s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [10:45<12:55, 45.63s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [11:30<12:08, 45.55s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [12:16<11:23, 45.59s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [13:02<10:39, 45.70s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [13:47<09:52, 45.61s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [14:33<09:07, 45.59s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [15:18<08:21, 45.59s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [16:04<07:36, 45.66s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [16:50<06:51, 45.71s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [17:36<06:05, 45.64s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [18:21<05:19, 45.58s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [19:07<04:33, 45.60s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [19:52<03:48, 45.62s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [20:38<03:02, 45.62s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [21:24<02:16, 45.59s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [22:09<01:31, 45.64s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [22:55<00:45, 45.58s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [23:41<00:00, 45.84s/it]\u001b[A\n",
      "rounds:  50%|█████     | 5/10 [1:34:36<1:38:46, 1185.33s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [01:14<37:12, 74.43s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [02:16<34:12, 70.77s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [03:18<31:47, 68.13s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [04:21<29:57, 66.56s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [05:24<28:19, 65.37s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [06:27<27:00, 64.80s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [07:30<25:40, 64.17s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [08:32<24:23, 63.63s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [09:35<23:13, 63.32s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [10:38<22:07, 63.23s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [11:41<21:07, 63.35s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [12:45<20:05, 63.44s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [13:50<19:11, 63.97s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [14:53<18:01, 63.62s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [15:55<16:51, 63.23s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [16:58<15:45, 63.02s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [18:01<14:40, 62.89s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [19:03<13:36, 62.81s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [20:06<12:33, 62.78s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [21:09<11:30, 62.79s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [22:11<10:26, 62.66s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [23:14<09:23, 62.60s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [24:17<08:21, 62.73s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [25:19<07:19, 62.79s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [26:23<06:17, 62.94s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [27:26<05:15, 63.03s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [28:29<04:12, 63.02s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [29:32<03:09, 63.15s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [30:36<02:06, 63.25s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [31:40<01:03, 63.45s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [32:44<00:00, 63.36s/it]\u001b[A\n",
      "rounds:  60%|██████    | 6/10 [2:07:21<1:34:35, 1418.96s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [01:03<31:57, 63.90s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [02:00<29:46, 61.59s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [02:55<27:53, 59.78s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [03:51<26:20, 58.53s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [04:47<25:00, 57.71s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [05:42<23:49, 57.17s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [06:38<22:39, 56.65s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [07:33<21:33, 56.23s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [08:28<20:30, 55.92s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [09:24<19:31, 55.79s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [10:19<18:32, 55.61s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [11:15<17:35, 55.57s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [12:10<16:38, 55.48s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [13:05<15:41, 55.39s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [14:00<14:45, 55.36s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [14:55<13:49, 55.31s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [15:51<12:54, 55.32s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [16:46<11:59, 55.34s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [17:42<11:04, 55.35s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [18:37<10:08, 55.29s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [19:32<09:12, 55.23s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [20:27<08:16, 55.19s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [21:22<07:21, 55.20s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [22:17<06:26, 55.19s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [23:13<05:31, 55.21s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [24:08<04:35, 55.20s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [25:03<03:40, 55.18s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [25:58<02:45, 55.27s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [26:54<01:50, 55.35s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [27:49<00:55, 55.36s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [28:45<00:00, 55.66s/it]\u001b[A\n",
      "rounds:  70%|███████   | 7/10 [2:36:06<1:15:32, 1510.88s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [01:05<32:35, 65.17s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [02:01<30:15, 62.60s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [02:58<28:25, 60.92s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [03:55<26:47, 59.52s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [04:51<25:25, 58.67s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [05:47<24:08, 57.95s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [06:44<23:00, 57.52s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [07:41<21:57, 57.27s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [08:37<20:55, 57.07s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [09:34<19:57, 57.03s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [10:31<18:58, 56.90s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [11:27<17:59, 56.80s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [12:24<17:01, 56.77s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [13:21<16:03, 56.70s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [14:18<15:09, 56.83s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [15:15<14:14, 56.98s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [16:12<13:19, 57.08s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [17:10<12:23, 57.22s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [18:07<11:26, 57.24s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [19:05<10:30, 57.29s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [20:03<09:35, 57.50s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [21:00<08:36, 57.43s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [21:57<07:38, 57.36s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [22:54<06:41, 57.33s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [23:51<05:43, 57.27s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [24:49<04:46, 57.26s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [25:46<03:49, 57.35s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [26:44<02:52, 57.48s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [27:42<01:55, 57.58s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [28:39<00:57, 57.48s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [29:37<00:00, 57.32s/it]\u001b[A\n",
      "rounds:  80%|████████  | 8/10 [3:05:43<53:01, 1590.73s/it]  \n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:56<28:15, 56.52s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [01:45<26:13, 54.25s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [02:34<24:31, 52.55s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [03:22<23:04, 51.29s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [04:10<21:52, 50.47s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [04:59<20:44, 49.77s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [05:47<19:42, 49.28s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [06:35<18:45, 48.94s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [07:24<17:57, 48.97s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [08:12<17:03, 48.76s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [09:01<16:17, 48.87s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [09:48<15:16, 48.26s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [10:35<14:23, 47.95s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [11:24<13:40, 48.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thresh:  48%|████▊     | 15/31 [12:13<12:51, 48.24s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [13:01<12:03, 48.24s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [13:50<11:17, 48.40s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [14:39<10:31, 48.57s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [15:27<09:43, 48.63s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [16:16<08:54, 48.59s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [17:05<08:06, 48.68s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [17:54<07:19, 48.81s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [18:42<06:29, 48.71s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [19:30<05:39, 48.45s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [20:19<04:51, 48.58s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [21:07<04:02, 48.55s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [21:56<03:13, 48.46s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [22:44<02:25, 48.41s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [23:32<01:36, 48.41s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [24:20<00:48, 48.28s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [25:09<00:00, 48.68s/it]\u001b[A\n",
      "rounds:  90%|█████████ | 9/10 [3:30:52<26:06, 1566.22s/it]\n",
      "thresh:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "thresh:   3%|▎         | 1/31 [00:26<13:29, 26.97s/it]\u001b[A\n",
      "thresh:   6%|▋         | 2/31 [00:44<11:42, 24.22s/it]\u001b[A\n",
      "thresh:  10%|▉         | 3/31 [01:02<10:24, 22.32s/it]\u001b[A\n",
      "thresh:  13%|█▎        | 4/31 [01:20<09:24, 20.93s/it]\u001b[A\n",
      "thresh:  16%|█▌        | 5/31 [01:37<08:38, 19.94s/it]\u001b[A\n",
      "thresh:  19%|█▉        | 6/31 [01:55<08:03, 19.34s/it]\u001b[A\n",
      "thresh:  23%|██▎       | 7/31 [02:13<07:34, 18.93s/it]\u001b[A\n",
      "thresh:  26%|██▌       | 8/31 [02:32<07:09, 18.69s/it]\u001b[A\n",
      "thresh:  29%|██▉       | 9/31 [02:50<06:47, 18.52s/it]\u001b[A\n",
      "thresh:  32%|███▏      | 10/31 [03:08<06:26, 18.42s/it]\u001b[A\n",
      "thresh:  35%|███▌      | 11/31 [03:34<06:52, 20.61s/it]\u001b[A\n",
      "thresh:  39%|███▊      | 12/31 [03:55<06:36, 20.89s/it]\u001b[A\n",
      "thresh:  42%|████▏     | 13/31 [04:16<06:16, 20.92s/it]\u001b[A\n",
      "thresh:  45%|████▌     | 14/31 [04:37<05:54, 20.88s/it]\u001b[A\n",
      "thresh:  48%|████▊     | 15/31 [04:57<05:29, 20.58s/it]\u001b[A\n",
      "thresh:  52%|█████▏    | 16/31 [05:17<05:06, 20.43s/it]\u001b[A\n",
      "thresh:  55%|█████▍    | 17/31 [05:37<04:44, 20.33s/it]\u001b[A\n",
      "thresh:  58%|█████▊    | 18/31 [05:57<04:23, 20.24s/it]\u001b[A\n",
      "thresh:  61%|██████▏   | 19/31 [06:17<04:01, 20.14s/it]\u001b[A\n",
      "thresh:  65%|██████▍   | 20/31 [06:37<03:41, 20.10s/it]\u001b[A\n",
      "thresh:  68%|██████▊   | 21/31 [06:57<03:19, 19.97s/it]\u001b[A\n",
      "thresh:  71%|███████   | 22/31 [07:16<02:59, 19.94s/it]\u001b[A\n",
      "thresh:  74%|███████▍  | 23/31 [07:36<02:39, 19.95s/it]\u001b[A\n",
      "thresh:  77%|███████▋  | 24/31 [07:56<02:19, 19.87s/it]\u001b[A\n",
      "thresh:  81%|████████  | 25/31 [08:16<01:59, 19.85s/it]\u001b[A\n",
      "thresh:  84%|████████▍ | 26/31 [08:36<01:39, 19.84s/it]\u001b[A\n",
      "thresh:  87%|████████▋ | 27/31 [08:55<01:19, 19.82s/it]\u001b[A\n",
      "thresh:  90%|█████████ | 28/31 [09:15<00:59, 19.85s/it]\u001b[A\n",
      "thresh:  94%|█████████▎| 29/31 [09:35<00:39, 19.81s/it]\u001b[A\n",
      "thresh:  97%|█████████▋| 30/31 [09:55<00:19, 19.86s/it]\u001b[A\n",
      "thresh: 100%|██████████| 31/31 [10:15<00:00, 19.85s/it]\u001b[A\n",
      "rounds: 100%|██████████| 10/10 [3:41:07<00:00, 1326.78s/it]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(metadata_versions)), desc='rounds'):\n",
    "    for thresh in tqdm(np.arange(0.5, 0.8, 0.01), desc='thresh'):\n",
    "        if not (k, thresh) in cache:\n",
    "            cache[(k, thresh)] = simulation(k, thresh)\n",
    "# res, err1, err2, mae1, mae2, pear1, pear2 = cache[(k, thresh)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, err1, err2, mae1, mae2, pear1, pear2 = simulation(0, 0.7, common_area_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28130264812258565,\n",
       " 0.49165013455619816,\n",
       " 19.44,\n",
       " 52.36,\n",
       " 0.4160858598766494,\n",
       " 0.3745000712544165)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err1, err2, mae1, mae2, pear1, pear2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error1</th>\n",
       "      <th>error2</th>\n",
       "      <th>MAE1</th>\n",
       "      <th>MAE2</th>\n",
       "      <th>pearson1</th>\n",
       "      <th>pearson2</th>\n",
       "      <th>rounds</th>\n",
       "      <th>thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.245305</td>\n",
       "      <td>0.252182</td>\n",
       "      <td>22.787234</td>\n",
       "      <td>24.634043</td>\n",
       "      <td>0.893370</td>\n",
       "      <td>0.887298</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190968</td>\n",
       "      <td>0.220638</td>\n",
       "      <td>16.803333</td>\n",
       "      <td>19.656667</td>\n",
       "      <td>0.934270</td>\n",
       "      <td>0.936309</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.176262</td>\n",
       "      <td>0.236198</td>\n",
       "      <td>19.008368</td>\n",
       "      <td>27.209205</td>\n",
       "      <td>0.918507</td>\n",
       "      <td>0.923513</td>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.208911</td>\n",
       "      <td>0.275857</td>\n",
       "      <td>24.474286</td>\n",
       "      <td>32.057143</td>\n",
       "      <td>0.898277</td>\n",
       "      <td>0.903590</td>\n",
       "      <td>3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197699</td>\n",
       "      <td>0.187196</td>\n",
       "      <td>19.944444</td>\n",
       "      <td>19.022222</td>\n",
       "      <td>0.931211</td>\n",
       "      <td>0.942359</td>\n",
       "      <td>4</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.157523</td>\n",
       "      <td>0.188820</td>\n",
       "      <td>16.217877</td>\n",
       "      <td>19.078212</td>\n",
       "      <td>0.951856</td>\n",
       "      <td>0.954051</td>\n",
       "      <td>5</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.189718</td>\n",
       "      <td>0.183328</td>\n",
       "      <td>18.239583</td>\n",
       "      <td>19.489583</td>\n",
       "      <td>0.873427</td>\n",
       "      <td>0.896040</td>\n",
       "      <td>6</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.189202</td>\n",
       "      <td>0.266478</td>\n",
       "      <td>18.720207</td>\n",
       "      <td>28.207254</td>\n",
       "      <td>0.917724</td>\n",
       "      <td>0.917517</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.123146</td>\n",
       "      <td>0.140305</td>\n",
       "      <td>13.012579</td>\n",
       "      <td>14.402516</td>\n",
       "      <td>0.950494</td>\n",
       "      <td>0.952319</td>\n",
       "      <td>8</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.268422</td>\n",
       "      <td>0.275529</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>0.924130</td>\n",
       "      <td>0.937187</td>\n",
       "      <td>9</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     error1    error2       MAE1       MAE2  pearson1  pearson2  rounds  \\\n",
       "0  0.245305  0.252182  22.787234  24.634043  0.893370  0.887298       0   \n",
       "1  0.190968  0.220638  16.803333  19.656667  0.934270  0.936309       1   \n",
       "2  0.176262  0.236198  19.008368  27.209205  0.918507  0.923513       2   \n",
       "3  0.208911  0.275857  24.474286  32.057143  0.898277  0.903590       3   \n",
       "4  0.197699  0.187196  19.944444  19.022222  0.931211  0.942359       4   \n",
       "5  0.157523  0.188820  16.217877  19.078212  0.951856  0.954051       5   \n",
       "6  0.189718  0.183328  18.239583  19.489583  0.873427  0.896040       6   \n",
       "7  0.189202  0.266478  18.720207  28.207254  0.917724  0.917517       7   \n",
       "8  0.123146  0.140305  13.012579  14.402516  0.950494  0.952319       8   \n",
       "9  0.268422  0.275529  21.600000  22.700000  0.924130  0.937187       9   \n",
       "\n",
       "   thresh  \n",
       "0    0.54  \n",
       "1    0.53  \n",
       "2    0.53  \n",
       "3    0.55  \n",
       "4    0.55  \n",
       "5    0.59  \n",
       "6    0.76  \n",
       "7    0.75  \n",
       "8    0.73  \n",
       "9    0.72  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [ 'error1', 'error2', 'MAE1', 'MAE2', 'pearson1', 'pearson2', 'rounds', 'thresh']\n",
    "data = []\n",
    "for k in cache:\n",
    "    metric = list(cache[k])\n",
    "    data.append(metric[1:] + list(k))\n",
    "metric_df = pd.DataFrame(data=data, columns=columns)\n",
    "metric_df = metric_df.groupby(['rounds']).apply(lambda rows: rows.sort_values('error1').head(1)).reset_index(drop=True)\n",
    "#metric_df.groupby(['rounds', 'thresh']).apply(lambda rows: rows[rows['err1'] == rows['error1'].min()])\n",
    "# up = metric_df[(metric_df['thresh'] == 0.54) & (metric_df['rounds'] <= 5)]\n",
    "# down = metric_df[(metric_df['thresh'] == 0.72) & (metric_df['rounds'] > 5)]\n",
    "# pd.concat([up, down], axis=0).reset_index(drop=True)\n",
    "\n",
    "metric_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keycard to device graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan = 30\n",
    "def session(day_keycard_df, day_ap_df): \n",
    "    graph = {}\n",
    "    for _, row in day_keycard_df.iterrows():\n",
    "        uid = row['uuid']\n",
    "        if not uid in graph:\n",
    "            graph[uid] = defaultdict(int)\n",
    "        timestamp_1st = row['localTimestamp']-timedelta(minutes=5)\n",
    "        device_1st_df = day_ap_df[day_ap_df['localTimestamp'] < timestamp_1st]\n",
    "        MAX_TIME = day_ap_df.iloc[0]['date'].replace(hour=23, minute=59, second=59)\n",
    "        timestamp_2nd = min(MAX_TIME, timestamp_1st + timedelta(minutes=timespan))\n",
    "        cond = (day_ap_df['localTimestamp'] > timestamp_1st) & (day_ap_df['localTimestamp'] < timestamp_2nd)\n",
    "        device_2nd_df = day_ap_df[cond]\n",
    "        bag1 = device_1st_df.uuid.unique()\n",
    "        bag2 = device_2nd_df.uuid.unique() \n",
    "    #     if first:\n",
    "    #         for i in bag1:\n",
    "    #             if i in graph[uuid]:\n",
    "    #                 del graph[uuid][i]\n",
    "        for i in bag2:\n",
    "            if (i in graph) or (not i in bag1):\n",
    "                graph[uid][i] += 1\n",
    "    return graph\n",
    "\n",
    "    \n",
    "def gen_keycard_device_graph(good_ap_df, valid_keycard_df):\n",
    "    pool = Pool(8)\n",
    "    valid_keycard_df = valid_keycard_df.sort_values('localTimestamp').reset_index(drop=True)\n",
    "    res = []\n",
    "    for key, day_keycard_df in valid_keycard_df.groupby(['date', 'floor_number']):\n",
    "        date, fl = key\n",
    "        day_ap_df = good_ap_df[(good_ap_df['date'] == date) & (good_ap_df['floor_number'] == int(fl))]\n",
    "        print (key, len(day_keycard_df), len(day_ap_df))\n",
    "        if len(day_ap_df) > 0:\n",
    "            res.append(pool.apply_async(func=session, args=(day_keycard_df, day_ap_df)))\n",
    "    print ('wait results ... ')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print ('finished !')\n",
    "    return res\n",
    "\n",
    "def gen_prob(kd_graph):\n",
    "    kv, dv = {}, {}\n",
    "    for kid in kd_graph:\n",
    "        if not kid in kv:\n",
    "            kv[kid] = len(kv)\n",
    "        for did in kd_graph[kid]:\n",
    "            if not did in dv:\n",
    "                dv[did] = len(dv)\n",
    "    cocur = np.zeros((len(kv), len(dv)))\n",
    "    kcur = np.zeros((len(kv), 1))\n",
    "    dcur = np.zeros((1, len(dv)))\n",
    "    for kid in kd_graph:\n",
    "        for did in kd_graph[kid]:\n",
    "            #kcur[kv[kid], 0] += kd_graph[kid][did]\n",
    "            cocur[kv[kid], dv[did]] = kd_graph[kid][did]\n",
    "            #dcur[0, dv[did]] += kd_graph[kid][did]\n",
    "    user_probs = cocur / (np.sum(cocur, axis=0, keepdims=True)+1e-3)\n",
    "    device_probs = cocur / (np.sum(cocur, axis=1, keepdims=True)+1e-3)\n",
    "    return user_probs, device_probs, kv, dv\n",
    "\n",
    "def soft_count(df, model, count_thresh, stationary_group):\n",
    "    probs, kv, dv = model\n",
    "    dids = df.uuid.unique()  \n",
    "    candis = np.array([int(dv[i]) for i in dids if i in dv])\n",
    "    #new_devices = [i for i in dids if (not i in dv) and (not i in stationary_group)]\n",
    "#     print (len(new_devices))\n",
    "    if len(candis) == 0:\n",
    "        return 0\n",
    "    # remove noise\n",
    "    candis_probs = probs[:, candis]\n",
    "#     print (len(np.where((candis_probs < 0.1) & (candis_probs > 0))[0]))\n",
    "#     candis_probs[candis_probs < 0.05] = 0\n",
    "    user = np.argmax(candis_probs, axis=1)\n",
    "    user_prob = np.max(candis_probs, axis=1)\n",
    "    valid_user = user[user_prob > 0]\n",
    "#     candis_probs = candis_probs / (np.sum(candis_probs, axis=0, keepdims=True)+1e-3)\n",
    "#     count = np.sum(candis_probs, axis=1)\n",
    "#     count[count > count_thresh] = 1\n",
    "#     count = np.sum(count)\n",
    "    return len(np.unique(user))\n",
    "\n",
    "def filter_low_rssi(valid_ap_df):\n",
    "    rssi = -80\n",
    "    rssi_cond = (valid_ap_df['rssi'] < 0) & (valid_ap_df['rssi'] > rssi)\n",
    "    return valid_ap_df[rssi_cond]\n",
    "\n",
    "def filter_weekend(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['dow'] = df.date.apply(lambda x: x.weekday())\n",
    "    return df[df.dow < 5]\n",
    "\n",
    "\n",
    "def preprocess(ap_df, keycard_df=None):\n",
    "    key_columns = ['localTimestamp', 'floor_number', 'uuid', 'apMac']\n",
    "    print ('before preprocess %d' % len(ap_df))\n",
    "    ap_df = filter_low_rssi(ap_df)\n",
    "    \n",
    "    ap_df = ap_df[~ap_df.duplicated(key_columns)]\n",
    "    stationary_group = gen_stationary_group(ap_df, keycard_df)\n",
    "    ap_df = ap_df[~ap_df.uuid.isin(stationary_group)]\n",
    "    print ('after preprocess %d' % len(ap_df))\n",
    "    return ap_df, stationary_group\n",
    "\n",
    "def simulation(valid_keycard_df, valid_ap_df, count_thresh):\n",
    "    #overlap, vocab = cooccurrence_model\n",
    "    good_ap_df = preprocess(valid_ap_df,valid_keycard_df)\n",
    "    kd_graph = gen_keycard_device_graph(good_ap_df, valid_keycard_df)\n",
    "    probs, kv, dv = gen_prob(kd_graph)\n",
    "    #uf = gen_device_groups(overlap, overlap_thresh)\n",
    "    metadata = {\n",
    "        'prob': probs,\n",
    "        #'keycard': valid_keycard_df,\n",
    "        'kv': kv,\n",
    "        'dv': dv,\n",
    "        #'uf': None,\n",
    "        'thresh': count_thresh,\n",
    "    }\n",
    "    \n",
    "    estimate = good_ap_df.groupby('date').apply(lambda df: soft_count(df, metadata))\n",
    "    truth = valid_keycard_df.groupby('date').apply(lambda rows: rows.uuid.nunique())\n",
    "#     print (good_ap_df.groupby('date').apply(lambda rows: len(rows.uuid.unique())))\n",
    "#     print (estimate)\n",
    "#     print (truth)\n",
    "    #return -abs(np.sum((estimate-truth) / truth) / len(truth) * 100)\n",
    "    return good_ap_df, estimate, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collector(results):\n",
    "    graph = {}\n",
    "    for _res in results:\n",
    "        res = _res.get()\n",
    "        merge_graph(graph, res)\n",
    "    return graph\n",
    "\n",
    "def merge_graph(graph_large, graph_small):\n",
    "    for uid in graph_small:\n",
    "        if not uid in graph_large:\n",
    "            graph_large[uid] = defaultdict(int)\n",
    "        for did in graph_small[uid]:\n",
    "            graph_large[uid][did] += graph_small[uid][did]\n",
    "    return graph_large\n",
    "\n",
    "def list_nodes(graph):\n",
    "    dv = set()\n",
    "    for kid in graph:\n",
    "        for did in graph[kid]:\n",
    "            dv.add(did)\n",
    "    return dv\n",
    "\n",
    "def delete_edges(graph, keycard_df, ap_df):\n",
    "    dates = keycard_df.date.unique()\n",
    "#     dv = list_nodes(graph)\n",
    "    for date in dates:\n",
    "        day_keycard_df = keycard_df[keycard_df.date == date]\n",
    "        day_ap_df = ap_df[ap_df.date == date]\n",
    "        kids = day_keycard_df.uuid.unique()\n",
    "        dids = day_ap_df.uuid.unique()\n",
    "        for uid in kids:\n",
    "            devices = list(graph[uid])\n",
    "            for did in devices:\n",
    "                if not did in dids:\n",
    "                    del graph[uid][did]\n",
    "    return graph\n",
    "\n",
    "# def delete_edges(graph, edges):\n",
    "#     for uid in graph:\n",
    "#         devices = list(graph[uid])\n",
    "#         for did in devices:\n",
    "#             if did in edges[uid]:\n",
    "#                 del graph[uid][did]\n",
    "#     return graph\n",
    "\n",
    "def delete_nodes(graph, nodes):\n",
    "    for uid in graph:\n",
    "        devices = list(graph[uid])\n",
    "        for did in devices:\n",
    "            if did in nodes:\n",
    "                del graph[uid][did]\n",
    "    return graph \n",
    "\n",
    "def build_graph(train_keycard_df, good_ap_df):\n",
    "    results = gen_keycard_device_graph(good_ap_df, train_keycard_df)\n",
    "    kd_graph = collector(results)\n",
    "    return kd_graph\n",
    "\n",
    "def clean_graph(graph, nodes, keycard_df, ap_df):\n",
    "    print ('before clean: %d' % sum([len(graph[i]) for i in graph]))\n",
    "    graph = delete_nodes(graph, nodes)\n",
    "    print ('after delete nodes: %d' % sum([len(graph[i]) for i in graph]))\n",
    "    graph = delete_edges(graph, keycard_df, ap_df)\n",
    "    print ('after delete edges: %d' % sum([len(graph[i]) for i in graph]))\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index():\n",
    "    date_start = datetime.datetime(2020, 1, 6)\n",
    "    date_end = datetime.datetime(2020, 3, 24)\n",
    "    date_groups = []\n",
    "    cur_date = date_start\n",
    "    while cur_date <= date_end:\n",
    "        date_groups.append((cur_date, min(date_end, cur_date+timedelta(days=6))))\n",
    "        cur_date += timedelta(days=7)\n",
    "    return date_groups\n",
    "\n",
    "def prepare_data(k, date_groups):\n",
    "    train_ap_df = valid_ap_df[(valid_ap_df.date >= date_groups[k][0]) & (valid_ap_df.date <= date_groups[k][1])]\n",
    "    test_ap_df = valid_ap_df[(valid_ap_df.date >= date_groups[k+1][0]) & (valid_ap_df.date <= date_groups[k+1][1])]\n",
    "    train_keycard_df = valid_keycard_df[(valid_keycard_df.date >= date_groups[k][0]) & (valid_keycard_df.date <= date_groups[k][1])]\n",
    "    test_keycard_df = valid_keycard_df[(valid_keycard_df.date >= date_groups[k+1][0]) & (valid_keycard_df.date <= date_groups[k+1][1])]\n",
    "    return train_ap_df, test_ap_df, train_keycard_df, test_keycard_df\n",
    "\n",
    "def update(graph_large, nodes_large, train_keycard_df, train_ap_df):\n",
    "    st = time.time()\n",
    "    train_ap_df = filter_weekend(train_ap_df)\n",
    "    train_keycard_df = filter_weekend(train_keycard_df)\n",
    "    good_ap_df, nodes_small = preprocess(train_ap_df, train_keycard_df)\n",
    "    good_ap_df = good_ap_df[~good_ap_df.uuid.isin(nodes_large)]\n",
    "    graph_small = build_graph(train_keycard_df, good_ap_df)\n",
    "    graph_small = clean_graph(graph_small, nodes_small, train_keycard_df, good_ap_df)\n",
    "    nodes = nodes_large.union(nodes_small)\n",
    "    print ('stationary nodes: %d' % len(nodes))\n",
    "    graph = merge_graph(graph_large, graph_small)\n",
    "    print ('running time %d'% (time.time()-st))\n",
    "    return graph, nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df = train_ap_df[~train_ap_df.duplicated(['uuid', 'floor_number', 'apMac', 'localTimestamp'])]\n",
    "occupancy = ap_df.groupby('uuid').apply(lambda df: df.groupby('date').apply(lambda rows: len(rows) / (24*60*2.0)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_groups = create_index()\n",
    "graph = {}\n",
    "block_nodes = set()\n",
    "for k in range(2):\n",
    "    train_ap_df, test_ap_df, train_keycard_df, test_keycard_df = prepare_data(k, date_groups)\n",
    "    graph, block_nodes = update(graph, block_nodes, train_keycard_df, train_ap_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sorted([len(graph[i]) for i in graph])\n",
    "c = 0\n",
    "for i in counts:\n",
    "    if i == 0:\n",
    "        c += 1\n",
    "print (c, len(counts) - c, max(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_keycard_df, test_ap_df, graph, stationary_group): \n",
    "    good_ap_df = test_ap_df[~test_ap_df.uuid.isin(stationary_group)]\n",
    "    probs, _, kv, dv = gen_prob(graph)\n",
    "    model_v1 = probs, kv, dv\n",
    "    estimate = good_ap_df.groupby('date').apply(lambda df: soft_count(df, model_v1, .5, stationary_group))\n",
    "    keycard = test_keycard_df.groupby('date').apply(lambda rows: rows.uuid.nunique())\n",
    "    delta = test_keycard_df.groupby('date').apply(lambda rows: rows[~rows.uuid.isin(kv)].uuid.nunique())\n",
    "    device = good_ap_df.groupby('date').apply(lambda rows: rows.uuid.nunique())\n",
    "  \n",
    "    dashboard = pd.concat([estimate, keycard, device, delta], axis=1).reset_index(drop=False)\n",
    "    dashboard = dashboard[~dashboard[0].isna()]\n",
    "    \n",
    "    #truth = pd.read_csv('japan/japan_manual_counts.csv')\n",
    "    return dashboard\n",
    "\n",
    "predict(test_keycard_df, test_ap_df, graph, block_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.read_csv('japan/japan_manual_counts.csv')\n",
    "truth = truth[truth['building_id'] == 22]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(truth, test_ap_df, graph, stationary_group):  \n",
    "    probs, _, kv, dv = gen_prob(graph)\n",
    "    model_v1 = probs, kv, dv\n",
    "    good_ap_df = test_ap_df[~test_ap_df.uuid.isin(stationary_group)]\n",
    "    truth['time_local'] = pd.to_datetime(truth['time_local'])\n",
    "    truth = truth[(truth['time_local'] >= test_ap_df.date.min()) & (truth['time_local'] <= test_ap_df.date.max())].reset_index(drop=True)\n",
    "    \n",
    "    for i, row in truth.iterrows():\n",
    "        timestamp = row['time_local']\n",
    "        cond = (good_ap_df['localTimestamp'] > (row['time_local'] - timedelta(minutes=5)))&(good_ap_df['localTimestamp'] < (row['time_local'] + timedelta(minutes=0)))\n",
    "        cond &=  (good_ap_df['floor_number'] == row['floor'])\n",
    "        ap_df = good_ap_df[cond]\n",
    "        truth.loc[i, 'device'] = ap_df.uuid.nunique()\n",
    "        truth.loc[i, 'estimate'] = soft_count(ap_df, model_v1, .5, stationary_group)\n",
    "        #print (ap_df.uuid.nunique(), row['manual_count'], count, timestamp, row['floor'])\n",
    "    return truth.sort_values(['floor', 'time_local']).reset_index(drop=True)\n",
    "res = evaluate(truth, test_ap_df, graph, block_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization    \n",
    "from bayes_opt import SequentialDomainReductionTransformer\n",
    "\n",
    "black_box_function = functools.partial(simulation, valid_keycard_df, valid_ap_df, model)\n",
    "pbounds = {'count_thresh': (0, 1)}\n",
    "bounds_transformer = SequentialDomainReductionTransformer()\n",
    "mutating_optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=1,\n",
    "    random_state=7,\n",
    "    bounds_transformer=bounds_transformer\n",
    ")\n",
    "st = time.time()\n",
    "mutating_optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=30,\n",
    ")\n",
    "print ('running time %d'% (time.time()-st))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
